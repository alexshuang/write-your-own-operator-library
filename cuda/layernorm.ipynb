{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda -q\n",
        "\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as drv\n",
        "import numpy as np\n",
        "from pycuda.compiler import SourceModule"
      ],
      "metadata": {
        "id": "v8iWGs461UV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layernorm 用于将张量在 channel 维度的值归一化为均值为0方差为1，核心计算在于计算均值和方差。gamma 和 beta 是可学习的参数:\n",
        "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2}} + \\beta$$\n"
      ],
      "metadata": {
        "id": "lrCZMmkVEM8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layernorm kernel（layernorm_fwd）采用比较直观的实现：一个 warp 负责计算一个 channel。将 thread block 划分成 warp groups，每个 warp 映射到输入输出张量上的一个 channel。"
      ],
      "metadata": {
        "id": "BsTqxRjVHyFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 4            # batch size\n",
        "n_seq = 512       # sequence len\n",
        "n_cxt = 1024      # max context len\n",
        "n_hidden = 768    # hidden size\n",
        "n_vocab = 50237   # vocab_size\n",
        "\n",
        "np.random.seed(42)\n",
        "gamma = np.random.randn(n_hidden).astype(np.float32)\n",
        "beta = np.random.randn(n_hidden).astype(np.float32)\n",
        "\n",
        "prog = SourceModule(\"\"\"\n",
        "#include <cooperative_groups.h>\n",
        "#include <cooperative_groups/reduce.h>\n",
        "\n",
        "namespace cg = cooperative_groups;\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void layernorm_fwd(float *out, const float *inp, const float *gamma, const float *beta, int B, int T, int C) {\n",
        "  cg::thread_block tb = cg::this_thread_block();\n",
        "  cg::thread_block_tile<32> warp = cg::tiled_partition<32>(tb);\n",
        "  int warp_idx = blockIdx.x * warp.meta_group_size() + warp.meta_group_rank();\n",
        "  int N = B * T;\n",
        "\n",
        "  const float *x = inp + warp_idx * C;\n",
        "\n",
        "  if (warp_idx < N) {\n",
        "    // mean\n",
        "    float sum = 0.0f;\n",
        "    for (int i = warp.thread_rank(); i < C; i += warp.num_threads()) {\n",
        "      sum += x[i];\n",
        "    }\n",
        "    float mean = cg::reduce(warp, sum, cg::plus<float>{}) / C;\n",
        "\n",
        "    // variance\n",
        "    float sum_sq = 0.0f;\n",
        "    for (int i = warp.thread_rank(); i < C; i += warp.num_threads()) {\n",
        "      float diff = x[i] - mean;\n",
        "      sum_sq += diff * diff;\n",
        "    }\n",
        "    float var = cg::reduce(warp, sum_sq, cg::plus<float>{}) / C;\n",
        "    var = rsqrt(var + 1e-5f);\n",
        "\n",
        "    // normalize\n",
        "    float *y = out + warp_idx * C;\n",
        "    for (int i = warp.thread_rank(); i < C; i += warp.num_threads()) {\n",
        "      float s = (__ldcs(x + i) - mean) * var;\n",
        "      y[i] = gamma[i] * s + beta[i];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\", no_extern_c=True)"
      ],
      "metadata": {
        "id": "CT_FQsuXjFdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "block size = 512，一个 block 计算 512/32 channels，一个 grid 需要 (bs * n_seq) / (block_size / 32) 个 blocks。"
      ],
      "metadata": {
        "id": "u3peVsGrf-FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layernorm_fwd = prog.get_function(\"layernorm_fwd\")\n",
        "out = np.empty((bs, n_seq, n_hidden), dtype=np.float32)\n",
        "input = np.random.randn(bs, n_seq, n_hidden).astype(np.float32)\n",
        "block_size = 512\n",
        "grid_size = int(np.ceil(bs * n_seq * 32 / block_size))\n",
        "layernorm_fwd(drv.Out(out), drv.In(input), drv.In(gamma), drv.In(beta), np.int32(bs), np.int32(n_seq), np.int32(n_hidden), block=(block_size,1,1), grid=(grid_size,1,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk7Ca32q7MS2",
        "outputId": "1af64417-10d3-44eb-80c7-95ac82831705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "验证计算结果："
      ],
      "metadata": {
        "id": "UfLIRC9hhJPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ref_layernorm_fwd(input, gamma, beta):\n",
        "  eps = 1e-5\n",
        "  mean = np.mean(input, axis=-1, keepdims=True)\n",
        "  variance = np.var(input, axis=-1, keepdims=True)\n",
        "  x = (input - mean) / np.sqrt(variance + eps)\n",
        "  return gamma * x + beta\n",
        "\n",
        "np.allclose(out, ref_layernorm_fwd(input, gamma, beta))"
      ],
      "metadata": {
        "id": "A3Ee8Fks9g_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659ceec5-b892-4209-96aa-d5b1522e61c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}